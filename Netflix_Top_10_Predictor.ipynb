{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Load and Prepare the Data"
      ],
      "metadata": {
        "id": "YGJxvv79icub"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_jQ_6ij6GGuY"
      },
      "source": [
        "## Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZblYsXtzGGuZ"
      },
      "outputs": [],
      "source": [
        "# Import our dependencies\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "\n",
        "# Import pandas and read the charity_data.csv from the provided cloud URL.\n",
        "import pandas as pd\n",
        "application_df = pd.read_csv(\"https://static.bc-edx.com/data/dl-1-2/m21/lms/starter/charity_data.csv\")\n",
        "application_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop columns?\n"
      ],
      "metadata": {
        "id": "FyuSzcOHJFKn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eauyuQilGGua"
      },
      "outputs": [],
      "source": [
        "# Determine the number of unique values in each column.\n",
        ".nunique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UNThkw-jGGua"
      },
      "outputs": [],
      "source": [
        "# Look at specific value counts to identify and replace with \"Other\"\n",
        "df[''].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ki846SW_GGua"
      },
      "outputs": [],
      "source": [
        "# # Choose a cutoff value and create a list of application types to be replaced\n",
        "# application_types_to_replace = ['T9', 'T13', 'T12', 'T2', 'T25', 'T14', 'T29', 'T15', 'T17']\n",
        "\n",
        "# # Replace in dataframe\n",
        "# for app in application_types_to_replace:\n",
        "#     application_df['APPLICATION_TYPE'] = application_df['APPLICATION_TYPE'].replace(app,\"Other\")\n",
        "\n",
        "# # Check to make sure replacement was successful\n",
        "# application_df['APPLICATION_TYPE'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vuJPnyO4GGuc"
      },
      "outputs": [],
      "source": [
        "# Convert categorical data to numeric with `pd.get_dummies`(won't change numerical columns, only categorical)\n",
        "categorical_dummies = pd.get_dummies(df)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Reuse this data for all the models\n",
        "# Split our preprocessed data into our features and target arrays\n",
        "df = pd.read_csv(\"\")\n",
        "X = df.drop(\"weeks_in_top_10\", axis=1)\n",
        "y = df[\"weeks_in_top_10\"]\n",
        "# Split the preprocessed data into a training and testing dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)"
      ],
      "metadata": {
        "id": "vuPSZbQWNsxF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QErw8kymGGuc"
      },
      "outputs": [],
      "source": [
        "# Create a StandardScaler instances\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# Fit the StandardScaler\n",
        "X_scaler = scaler.fit(X_train)\n",
        "\n",
        "# Scale the data\n",
        "X_train_scaled = X_scaler.transform(X_train)\n",
        "X_test_scaled = X_scaler.transform(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "classification_dummies.shape[1]"
      ],
      "metadata": {
        "id": "zLCGtqGCPSLf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Linear Regression Model\n",
        "\n",
        "We started with a **Linear Regression model** as a simple and interpretable baseline. This helped us:\n",
        "\n",
        "- Quickly test if a linear relationship exists between the features and the target\n",
        "- Understand which features have the strongest impact on the outcome\n",
        "- Provide an easy-to-explain model using coefficients\n",
        "- Compare performance against more complex models like Random Forest and Keras\n",
        "\n",
        "Even if it's not the best performer, it gives us a valuable starting point for analysis and model comparison."
      ],
      "metadata": {
        "id": "H-UyTz9xB8Sx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "import numpy as np\n",
        "\n",
        "# Create and fit the model\n",
        "lr = LinearRegression()\n",
        "lr.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Predict\n",
        "y_pred = lr.predict(X_test_scaled)\n",
        "\n",
        "# Evaluate\n",
        "mae = mean_absolute_error(y_test, y_pred)\n",
        "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "print(f\"Linear Regression Mean Absolute Error (mae): {mae:.2f}\")\n",
        "print(f\"Linear Regression Root Mean Squared Error: {rmse:.2f}\")\n",
        "print(f\"Linear Regression R2: {r2:.2f}\")\n"
      ],
      "metadata": {
        "id": "Ei8esaL0Pv83"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Random Forest Model\n",
        "\n",
        "We used a **Random Forest Regressor** to improve predictive performance over the linear model. This method:\n",
        "\n",
        "- Handles complex, nonlinear relationships without requiring feature engineering\n",
        "- Is robust to outliers and overfitting due to its use of multiple decision trees\n",
        "- Automatically detects feature importance and interactions\n",
        "- Requires minimal preprocessing and works well with both numeric and encoded categorical data\n",
        "\n",
        "It’s a strong, reliable model for tabular data and a valuable benchmark before moving to deep learning."
      ],
      "metadata": {
        "id": "LTlck6g8P-Ur"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import RandomForestRegressor\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "## Random forest DOES NOT use scaled data\n",
        "\n",
        "# Create and train the Random Forest model\n",
        "rf_model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "rf_model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred_rf = rf_model.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "mae_rf = mean_absolute_error(y_test, y_pred_rf)\n",
        "rmse_rf = np.sqrt(mean_squared_error(y_test, y_pred_rf))\n",
        "r2_rf = r2_score(y_test, y_pred_rf)\n",
        "\n",
        "print(f\"Random Forest Mean Absolute Error: {mae_rf:.2f}\")\n",
        "print(f\"Random Forest Root Mean Squared Error: {rmse_rf:.2f}\")\n",
        "print(f\"Random Forest R² Score: {r2_rf:.2f}\")\n"
      ],
      "metadata": {
        "id": "ePRDTyNXRfGl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Keras Neural Network Model\n",
        "\n",
        "We used a **Keras neural network** to explore whether deep learning can capture more complex patterns in the data. This model:\n",
        "\n",
        "- Learns nonlinear relationships and deep feature interactions\n",
        "- Can potentially outperform traditional models with enough data and tuning\n",
        "- Requires feature scaling and hyperparameter tuning for best results\n",
        "- Is highly customizable in terms of architecture and optimization\n",
        "\n",
        "This model helps us test if complexity leads to better predictive performance, especially after comparing it with simpler methods.\n"
      ],
      "metadata": {
        "id": "-8uI8RddBzPX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the Keras Model\n",
        "\n",
        "# This counts how many features (columns) your data has, so the model knows what kind of input to expect.\n",
        "number_input_features = len(X_train[0])\n",
        "\n",
        "hidden_nodes_layer1 = 80\n",
        "hidden_nodes_layer2 = 40\n",
        "hidden_nodes_layer3 = 40\n",
        "\n",
        "# Start building the Sequential model\n",
        "nn = tf.keras.models.Sequential()\n",
        "\n",
        "# First hidden layer\n",
        "nn.add(tf.keras.layers.Dense(units=hidden_nodes_layer1, input_dim= number_input_features, activation=\"relu\"))\n",
        "\n",
        "# Second hidden layer\n",
        "nn.add(tf.keras.layers.Dense(units= hidden_nodes_layer2, activation=\"relu\"))\n",
        "\n",
        "# Third Hidden layer\n",
        "nn.add(tf.keras.layers.Dense(units= hidden_nodes_layer3, activation=\"relu\"))\n",
        "\n",
        "# Output layer – Removed sigmoid because we are predicting a continuous number (weeks), not a binary value (0 or 1)\n",
        "nn.add(tf.keras.layers.Dense(units=1))\n",
        "\n",
        "# Check the structure of the model\n",
        "nn.summary()\n",
        "\n",
        "# Compile the model\n",
        "# loss='mse' directs the model to minimize mean squared error during training\n",
        "# metrics=['mae'] tracks the average prediction error (in weeks) for easier interpretation\n",
        "nn.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
        "\n",
        "# Train the model\n",
        "fit_model = nn.fit(X_train_scaled, y_train, epochs=50)\n"
      ],
      "metadata": {
        "id": "HU8xEFy5BluX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "# validation_split=0.2 reserves 20% of training data for validation, helping us monitor overfitting and track performance on unseen data.\n",
        "# batch_size=32 trains the model in small groups, balancing speed, memory use, and generalization.\n",
        "fit_model_history = nn.fit(X_train_scaled, y_train, validation_split=0.2, epochs=50, batch_size=32)\n"
      ],
      "metadata": {
        "id": "RVhplY19Cf1M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model on test data\n",
        "# MAE (Mean Absolute Error) shows how many weeks off the predictions are, on average\n",
        "loss, mae = nn.evaluate(X_test_scaled, y_test)\n",
        "print(f\"Test MAE: {mae:.2f}, Loss (MSE): {loss:.2f}\")\n"
      ],
      "metadata": {
        "id": "qEme1xcAB43t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lMUZtAPqGGuc"
      },
      "source": [
        "# Keras Tuner\n",
        "\n",
        "We used **Keras Tuner** to optimize our neural network’s architecture and hyperparameters. While our base Keras model gives us a starting point, Keras Tuner helps us:\n",
        "\n",
        "- Automatically search for the best combination of layers, units, activations, and learning rates\n",
        "- Improve model performance without manually guessing parameters\n",
        "- Reduce overfitting or underfitting by finding better model configurations\n",
        "- Explore multiple architectures efficiently\n",
        "\n",
        "This step helps us fine-tune our model and potentially outperform traditional machine learning methods by leveraging the flexibility of deep learning.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# This counts how many features (columns) your data has, so the model knows what kind of input to expect.\n",
        "number_input_features = len(X_train[0])\n",
        "\n",
        "# Create a method that creates a new Sequential model with hyperparameter options\n",
        "def create_model(hp):\n",
        "    nn = tf.keras.models.Sequential()\n",
        "\n",
        "    # Allow kerastuner to decide which activation function to use in hidden layers\n",
        "    activation = hp.Choice('activation',['relu','selu'])\n",
        "\n",
        "    # Allow kerastuner to decide number of neurons in first layer\n",
        "    nn.add(tf.keras.layers.Dense(units=hp.Int('first_units',\n",
        "        min_value=8,\n",
        "        max_value=128,\n",
        "        step=8), activation=activation, input_dim=number_input_features))\n",
        "\n",
        "    # Allow kerastuner to decide number of hidden layers and neurons in hidden layers\n",
        "    for i in range(hp.Int('num_layers', 1, 4)):\n",
        "        nn.add(tf.keras.layers.Dense(units=hp.Int('units_' + str(i),\n",
        "            min_value=8,\n",
        "            max_value=64,\n",
        "            step=8),\n",
        "            activation=activation))\n",
        "\n",
        "    # Output layer for regression (no activation)\n",
        "    nn.add(tf.keras.layers.Dense(units=1))\n",
        "\n",
        "    # Compile the model\n",
        "    nn.compile(loss=\"mse\", optimizer=\"adam\", metrics=[\"mae\"])\n",
        "\n",
        "    return nn"
      ],
      "metadata": {
        "id": "Wm_12o5ql5l3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install keras-tuner"
      ],
      "metadata": {
        "id": "98NFlp0_y0jt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import keras_tuner as kt\n",
        "\n",
        "tuner = kt.Hyperband(\n",
        "    create_model,\n",
        "    objective=\"val_mae\",         # Mean Absolute Error for regression\n",
        "    max_epochs=20,\n",
        "    hyperband_iterations=2)\n"
      ],
      "metadata": {
        "id": "V-4DEELoV151"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run the Keras Tuner search for best hyperparameters\n",
        "tuner.search(X_train_scaled, y_train,\n",
        "             epochs=20,\n",
        "             validation_split=0.2,\n",
        "             batch_size=32)\n"
      ],
      "metadata": {
        "id": "PVLYVTFHWVYh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get best model hyperparameters\n",
        "best_hyper = tuner.get_best_hyperparameters(1)[0]\n",
        "best_hyper.values"
      ],
      "metadata": {
        "id": "HQb-cDaph6r_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate best model against full test data\n",
        "best_model = tuner.get_best_models(1)[0]\n",
        "\n",
        "# Evaluate it on the test set\n",
        "loss, mae = best_model.evaluate(X_test_scaled, y_test)\n",
        "print(f\"Best Tuned Model - Test Mean Absolute Error: {mae:.2f}\")\n",
        "\n",
        "# Save the model to a file\n",
        "best_model.save('keras_tuner_model.keras')"
      ],
      "metadata": {
        "id": "lXvcXYlyQemM"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.-1.-1"
    },
    "vscode": {
      "interpreter": {
        "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
      }
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}